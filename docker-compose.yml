services:
  zookeeper:
    image: bitnami/zookeeper:3.8
    environment:
      - ALLOW_ANONYMOUS_LOGIN=yes
    ports:
      - "2181:2181"
    restart: always

  kafka:
    image: bitnami/kafka:3.5
    environment:
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper
    restart: always

  api:
    build: .
    container_name: api
    working_dir: /app
    environment:
      - PYTHONPATH=/app
      - DB_PATH=/app/data/rssfeed.duckdb
      - KAFKA_BOOTSTRAP=kafka:9092
      - OLLAMA_HOST=http://ollama:11434
      - LLM_CHAT_LOG_DIR=/app/logs
    volumes:
      - rssdata:/app/data
      - ./logs:/app/logs
    ports: ["5000:5000"]
    command: >
      bash -c "
        echo '→ RSS-Ingest starten';
        python -u -m ingestion.rss_ingest &&
        echo '→ Ingest fertig, starte API';
        python -u -m api.app
      "
    depends_on:
      - kafka
      - ollama
    restart: always

  ollama:
    image: ollama/ollama
    container_name: ollama
    runtime: nvidia
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    restart: always

  spark-analytics:
    image: bitnami/spark:3
    container_name: spark-analytics
    # Arbeitsverzeichnis auf den im Repo gemounteten Code
    working_dir: /app
    # Mount deines Projekt‑Roots sowie des gemeinsamen Volumes
    volumes:
      - ./:/app
      - rssdata:/data
    environment:
      # Damit Spark auf Kafka zugreifen kann
      - KAFKA_BOOTSTRAP=kafka:9092
    # Stream‑Job, der dauerhaft läuft
    command: >
      /opt/bitnami/spark/bin/spark-submit
      --master local[2]
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1
      /app/analytics/analytics_stream.py
    # Spark‑UI auf Port 4040 sichtbar
    ports:
      - "4040:4040"
    depends_on:
      - kafka
    restart: always

  spark-quality:
    image: bitnami/spark:3
    container_name: spark-quality
    working_dir: /app
    volumes:
      - ./:/app
      - rssdata:/data
    environment:
      - KAFKA_BOOTSTRAP=kafka:9092
    command: >
      /opt/bitnami/spark/bin/spark-submit
      --master local[2]
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.1
      /app/analytics/spark_quality_checks.py
    # Für den Fall, dass auch dieser Job eine UI startet (z.B. Port 4041)
    ports:
      - "4041:4040"
    depends_on:
      - kafka
    restart: always

volumes:
  rssdata:
  ollama: